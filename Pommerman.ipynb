{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c071a3e3-e12d-47da-ae56-0745c49cf5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auto-format notebook\n",
    "# %load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc45c92-e9b9-496b-a066-962dc82bdb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/raphael/uni/jku/2_sem/reinforcement_learning/assignments', '/home/raphael/.pyenv/versions/3.7.12/lib/python37.zip', '/home/raphael/.pyenv/versions/3.7.12/lib/python3.7', '/home/raphael/.pyenv/versions/3.7.12/lib/python3.7/lib-dynload', '', '/home/raphael/.local/share/virtualenvs/assignments-geAH-P9e/lib/python3.7/site-packages', '/home/raphael/.local/share/virtualenvs/assignments-geAH-P9e/lib/python3.7/site-packages/IPython/extensions', '/home/raphael/.ipython']\n",
      "/home/raphael/.local/share/virtualenvs/assignments-geAH-P9e/lib/python3.7/site-packages\n",
      "['/home/raphael/.local/share/virtualenvs/assignments-geAH-P9e/lib/python3.7/site-packages', '/home/raphael/uni/jku/2_sem/reinforcement_learning/assignments', '/home/raphael/.pyenv/versions/3.7.12/lib/python37.zip', '/home/raphael/.pyenv/versions/3.7.12/lib/python3.7', '/home/raphael/.pyenv/versions/3.7.12/lib/python3.7/lib-dynload', '', '/home/raphael/.local/share/virtualenvs/assignments-geAH-P9e/lib/python3.7/site-packages', '/home/raphael/.local/share/virtualenvs/assignments-geAH-P9e/lib/python3.7/site-packages/IPython/extensions', '/home/raphael/.ipython']\n",
      "GraphicPommeFFAFast-v0 env is registered.\n",
      "GraphicOneVsOne-v0 env is registered.\n",
      "GraphicOVOCompact-v0 env is registered.\n",
      "GraphicOVONano-v0 env is registered.\n",
      "Hint: just ignore the error \"Import error NSDE! You will not be able to render --> Cannot connect to 'None'\"\n",
      "PommeFFACompetition-v0\n",
      "PommeFFACompetitionFast-v0\n",
      "PommeFFAFast-v0\n",
      "PommeFFA-v1\n",
      "PommeRadioCompetition-v2\n",
      "PommeRadio-v2\n",
      "PommeTeamCompetition-v0\n",
      "PommeTeamCompetitionFast-v0\n",
      "PommeTeamCompetition-v1\n",
      "PommeTeam-v0\n",
      "PommeTeamFast-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0 Stop\\n1 Up\\n2 Down\\n3 Left\\n4 Right\\n5 Bomb\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "import shutil\n",
    "from time import strftime, time\n",
    "from collections import deque, namedtuple\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "\n",
    "import gym\n",
    "from gym import Env, Wrapper\n",
    "\n",
    "from pommerman import make\n",
    "from pommerman.agents import BaseAgent, RandomAgent, SimpleAgent\n",
    "from graphic_pomme_env import graphic_pomme_env\n",
    "from graphic_pomme_env.wrappers import PommerEnvWrapperFrameSkip2\n",
    "\n",
    "print(\n",
    "    '''Hint: just ignore the error \"Import error NSDE! You will not be able to render --> Cannot connect to 'None'\"'''\n",
    ")\n",
    "pomenvs = [es.id for es in gym.envs.registry.all() if es.id.startswith(\"Pomme\")]\n",
    "print(\"\\n\".join(pomenvs))\n",
    "res = graphic_pomme_env.load_resources()\n",
    "N_PLAYERS = 2\n",
    "NUM_STACK = 5\n",
    "\n",
    "NUM_ACTIONS = 6\n",
    "\"\"\"\n",
    "0 Stop\n",
    "1 Up\n",
    "2 Down\n",
    "3 Left\n",
    "4 Right\n",
    "5 Bomb\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a5bfb8-a0e9-4d58-bb7d-b29953d46135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34687a0c-631a-4031-bd66-7654feccd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc7997-ba3d-465a-b6ed-6d2d601d70ab",
   "metadata": {},
   "source": [
    "# Plan to train agent\n",
    "\n",
    "1. train agains static agent\n",
    "2. train against random no bomb agent\n",
    "3. train againts random bomb agent\n",
    "4. train against itself\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10536f9f-9d93-4aac-bfca-c890eedb8d68",
   "metadata": {},
   "source": [
    "## Define ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12003bfb-cc57-4181-a6b8-fbf89d6ec833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class Transition:\n",
    "#     state: Any\n",
    "#     action: int\n",
    "#     reward: float\n",
    "#     next_state: Any\n",
    "#     done: bool\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Idea to implement:\n",
    "If agent wins we will add the rewards of all the steps which led to this win.\n",
    "If the agent looses we will add the loss to all the steps which led to this loss.\n",
    "\n",
    "Given the agent plants a bomb 10 timesteps ahead of the win - it should get more reward on this action.\n",
    "Otherwise there are multiple options:\n",
    "- equally split the rewards over all the steps in this episode\n",
    "- give reward to the n steps before the win / loss\n",
    "- decrease the rewards over n or all timesteps\n",
    "\n",
    "Problem:\n",
    "- some states might be the same -> will therefore be higher and lower in the end\n",
    "- need to figure out if this happend in the same episode: DONE\n",
    "- need to figure out a good value for the reward since 200 might be too much\n",
    "- need to find a good size of n\n",
    "\n",
    "\n",
    "\n",
    "Implement the equal over n episodes\n",
    "if the agent wins - it maybe approached the opponent and placed a bomb and then got out of the way\n",
    "so 10 steps bomb + 10 steps approaching should be good\n",
    "\n",
    "but if the agent looses\n",
    "not too much steps behind are bad\n",
    "in most cases it should take the agent 2 steps to get away from a bomb -> I add a extra one to be sure\n",
    "so 4 steps will be for looses\n",
    "\n",
    "draws aren't really supportet yet :P we use the loose stuff since he gets in the way of the bomb and there aren't any pos rewards anyway\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, num_actions, size=10_000, remove_low_reward=0.0):\n",
    "        self.size = size\n",
    "        self.transition = []\n",
    "        self.num_actions = num_actions\n",
    "        self.remove_low_reward = remove_low_reward\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        state,\n",
    "        action,\n",
    "        reward,\n",
    "        next_state,\n",
    "        done,\n",
    "        episode,\n",
    "        split_over_win=30,\n",
    "        split_over_loose=4,\n",
    "    ):\n",
    "        if self.length() > self.size:\n",
    "            self.remove()\n",
    "\n",
    "        if reward > 0:\n",
    "            split_over = split_over_win\n",
    "        else:\n",
    "            split_over = split_over_loose\n",
    "\n",
    "        if done and self.length() > split_over and reward > 0:\n",
    "            reward = 20\n",
    "            helper = list(filter(lambda x: x[5] == episode, self.transition))[\n",
    "                -split_over:\n",
    "            ]\n",
    "\n",
    "            helper = [\n",
    "                [\n",
    "                    el[0],\n",
    "                    el[1],\n",
    "                    el[2] + (reward * 2) / (len(helper) * 2 - i),\n",
    "                    el[3],\n",
    "                    el[4],\n",
    "                    el[5],\n",
    "                ]\n",
    "                for i, el in enumerate(helper)\n",
    "            ]\n",
    "\n",
    "            for idx in range(-11, -8):\n",
    "                if abs(idx) <= len(helper):\n",
    "                    if helper[idx][1] == 5:\n",
    "                        helper[idx][2] = helper[idx][2] + 10\n",
    "\n",
    "        elif done and self.length() > split_over and reward <= 0:\n",
    "            reward = -20\n",
    "            helper = list(filter(lambda x: x[5] == episode, self.transition))[\n",
    "                -split_over:\n",
    "            ]\n",
    "\n",
    "            r_plus = reward / len(helper)\n",
    "\n",
    "            helper = [\n",
    "                [el[0], el[1], el[2] + r_plus, el[3], el[4], el[5]] for el in helper\n",
    "            ]\n",
    "\n",
    "            self.transition[-len(helper) :] = helper\n",
    "\n",
    "        self.transition.append([state, action, reward, next_state, done, episode])\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.transition)\n",
    "\n",
    "    def remove(self):\n",
    "        idx = 0\n",
    "        if np.random.random() < self.remove_low_reward:\n",
    "\n",
    "            lowest_reward = min(self.transition, key=lambda x: x[2])[2]\n",
    "\n",
    "            for i, t in enumerate(self.transition):\n",
    "                if t[2] == lowest_reward:\n",
    "                    self.transition.pop(i)\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            self.transition.pop(idx)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = random.sample(self.transition, batch_size)\n",
    "        states, actions, rewards, next_states, dones, _ = map(np.array, zip(*batch))\n",
    "        actions = torch.from_numpy(actions.astype(int)).to(device, dtype=torch.int)\n",
    "        rewards = torch.from_numpy(rewards).to(device, dtype=torch.float32)\n",
    "        dones = torch.from_numpy(dones.astype(int)).to(device, dtype=torch.int)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def get_action_distribution(self):\n",
    "        states, actions, rewards, next_states, dones, _ = map(\n",
    "            np.array, zip(*self.transition)\n",
    "        )\n",
    "\n",
    "        _lens = []\n",
    "        for i in range(0, 6):\n",
    "            _lens.append(len(list(filter(lambda x: x == i, actions))))\n",
    "\n",
    "        return _lens\n",
    "\n",
    "    def save_episode(self, episode):\n",
    "        eps = list(filter(lambda x: x[5] == episode, self.transition))\n",
    "        states, actions, rewards, next_states, dones, _ = map(\n",
    "            np.array, zip(*self.transition)\n",
    "        )\n",
    "\n",
    "        np.save(\n",
    "            f\"./demos/states_{datetime.now().strftime('%Y_%m_%d-%H_%M_%S')}.npy\",\n",
    "            states,\n",
    "            allow_pickle=True,\n",
    "        )\n",
    "        np.save(\n",
    "            f\"./demos/actions_{datetime.now().strftime('%Y_%m_%d-%H_%M_%S')}.npy\",\n",
    "            actions,\n",
    "            allow_pickle=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c4bb0-7248-4b41-bcc0-7dfacc8b4726",
   "metadata": {},
   "source": [
    "## Define cheap oponent agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42bb53be-5cc9-457c-a018-d66ffddbf23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_agent(frame_stack):\n",
    "    del frame_stack\n",
    "    return 0\n",
    "\n",
    "\n",
    "def rand_no_bomb_agent(frame_stack):\n",
    "    del frame_stack\n",
    "    return np.random.randint(NUM_ACTIONS - 1)\n",
    "\n",
    "\n",
    "def rand_agent(frame_stack):\n",
    "    return np.random.randint(NUM_ACTIONS)\n",
    "\n",
    "\n",
    "def model_agent(frame_stack, model):\n",
    "    with torch.no_grad():\n",
    "        obs = torch.from_numpy(np.array(frame_stack.get_obersvation()))\n",
    "        net_out = model(obs).detach().cpu().numpy()\n",
    "\n",
    "    action = np.argmax(net_out)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e81c9f-701b-4cbb-ba94-2eed3997dd99",
   "metadata": {},
   "source": [
    "## Define our Agent Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f38588d-305c-4433-a93e-4c012b09d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PommermanAgent(nn.Module):\n",
    "    def __init__(self, num_stack, num_actions=6):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(num_stack, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(305280, 64),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, num_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.array(x)\n",
    "        x = torch.Tensor(x).to(device)\n",
    "\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce7c6e6-a333-4194-bb67-99aa1d583c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_environment(oponent, random_env):\n",
    "    # since this is also used for evaluation I see no need to train for different values atm\n",
    "    env = PommerEnvWrapperFrameSkip2(num_stack=5, start_pos=0, opponent_actor=oponent)\n",
    "\n",
    "    if random_env:\n",
    "        n_rigid = np.random.randint(low=1, high=6) * 2\n",
    "        n_wood = np.random.randint(low=3, high=7) * 2\n",
    "        n_items = np.random.randint(low=1, high=n_wood)\n",
    "\n",
    "        env.set_board_params(num_rigid=n_rigid, num_wood=n_wood, num_items=n_items)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b74c1ef-8db2-4bfb-8ab2-8c2e90f46b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GraphicOVOCompact-v0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = get_random_environment(None, False)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d4053e-1610-42a0-8195-dbf7c70c9680",
   "metadata": {},
   "source": [
    "## Pre-Train our Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f03d6c1f-cde8-4842-ab71-e55c9e2b5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Demonstrations(Dataset):\n",
    "    def __init__(self, actions, states):\n",
    "        self.actions = actions\n",
    "        self.states = states\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.actions.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        state = self.states[idx]\n",
    "        action = self.actions[idx]\n",
    "\n",
    "        return state, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0ed469f-a8a4-4a60-ac05-53e7ed65421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2553,)\n",
      "(2553, 5, 56, 48)\n"
     ]
    }
   ],
   "source": [
    "loaded_actions = np.load(\"demo_actions.npy\", allow_pickle=True)\n",
    "loades_states = np.load(\"demo_states.npy\", allow_pickle=True)\n",
    "\n",
    "print(loaded_actions.shape)\n",
    "print(loades_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db9afd93-69f2-4b70-afee-df7b842a2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "demos = Demonstrations(loaded_actions, loades_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a0bf17-01b0-46c0-b93e-b7837933f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, demos, loss_func, optimizer, epoch):\n",
    "\n",
    "    net.train()\n",
    "    ts_len = len(demos)\n",
    "    alpha = 0.3\n",
    "\n",
    "    loader = DataLoader(\n",
    "        demos,\n",
    "        batch_size=batchsize,\n",
    "        num_workers=1,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    mean_losses = []\n",
    "\n",
    "    l_len = len(loader)\n",
    "    for j, (frame, action) in enumerate(loader):\n",
    "        frame = frame.numpy()\n",
    "        action = action.to(device)\n",
    "        # action = action.numpy()\n",
    "\n",
    "        prediction = net(frame)\n",
    "        loss = loss_func(prediction, action)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(prediction, dim=-1)\n",
    "            entropy = torch.mean(-torch.sum(probs * torch.log(probs), dim=-1))\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mean_losses.append(loss.item())\n",
    "\n",
    "    return sum(mean_losses) / len(mean_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94035718-3eff-49a2-8b5b-93658b729167",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "batchsize = 32\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a76a1b81-9514-480e-9b3d-6d9336c7c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = PommermanAgent(num_stack=5, num_actions=NUM_ACTIONS).to(device)\n",
    "# loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# optimizer = optim.Adam(agent.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# losses = []\n",
    "\n",
    "# with tqdm(range(n_epochs)) as pbar:\n",
    "#     for i_ep in pbar:\n",
    "#         running_loss = train(agent, demos, loss_func, optimizer, i_ep + 1)\n",
    "#         losses.append(running_loss)\n",
    "#         scheduler.step()\n",
    "\n",
    "#         pbar.set_description(\n",
    "#             f\"Epoch: \\t{i_ep}\\t{running_loss}\\t{datetime.now().strftime('%Y_%m_%d-%H_%M_%S')}\"\n",
    "#         )\n",
    "\n",
    "\n",
    "# checkpoint_dict = {\n",
    "#     \"model_params\": agent.state_dict(),\n",
    "#     \"timesteps\": 0,\n",
    "# }\n",
    "# torch.save(\n",
    "#     checkpoint_dict,\n",
    "#     \"./final_challenge/pre_trained_model.p\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21704618-b1c4-4345-90b3-d6a0e18c03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(losses)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170f223-0512-49f0-9a78-f331c4528e58",
   "metadata": {},
   "source": [
    "## Define our Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1fb2c0b-d85b-4fab-9e2d-c4c73d30bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        agent,\n",
    "        target_agent,\n",
    "        buffer,\n",
    "        optimizer,\n",
    "        loss,\n",
    "        eps_min,\n",
    "        eps_max,\n",
    "        eps_decay,\n",
    "        batch_size,\n",
    "        update_every,\n",
    "        gamma,\n",
    "        tau,\n",
    "        oponent,\n",
    "        random_env,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.agent = agent\n",
    "        self.target_agent = target_agent\n",
    "        self.buffer = buffer\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_max = eps_max\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.oponent = oponent\n",
    "        self.random_env = random_env\n",
    "\n",
    "        self.returns = []\n",
    "        self.losses = []\n",
    "\n",
    "    def soft_update(self):\n",
    "        \"\"\"\n",
    "        agent -> weights will be copied from\n",
    "        target_agent -> weights will be copied to\n",
    "        tau -> interpolation parameter\n",
    "        \"\"\"\n",
    "        for agent_param, target_agent_param in zip(\n",
    "            self.agent.parameters(), self.target_agent.parameters()\n",
    "        ):\n",
    "            target_agent_param.data.copy_(\n",
    "                tau * agent_param.data + (1 - tau) * target_agent_param.data\n",
    "            )\n",
    "\n",
    "    def run(self, timesteps, n_epochs):\n",
    "        returns = []\n",
    "        losses = []\n",
    "\n",
    "        env = get_random_environment(self.oponent, self.random_env)\n",
    "        agent_obs, opponent_obs = env.reset()\n",
    "        agent_obs = np.array(agent_obs).astype(np.uint8)\n",
    "        \n",
    "        ws = 0\n",
    "        ls = 0\n",
    "        ds = 0\n",
    "        \n",
    "\n",
    "        with tqdm(range(n_epochs)) as pbar:\n",
    "            for i in pbar:\n",
    "                ret = 0\n",
    "                done = False\n",
    "\n",
    "                bomb_timestep = []\n",
    "\n",
    "                while not done:\n",
    "                    eps = max(self.eps_min, self.eps_max - timesteps / self.eps_decay)\n",
    "\n",
    "                    if np.random.choice([0, 1], p=[1 - eps, eps]) == 1:\n",
    "                        a = np.random.randint(low=1, high=NUM_ACTIONS, size=1)[0]\n",
    "                    else:\n",
    "                        with torch.no_grad():\n",
    "                            self.agent.eval()\n",
    "                            net_out = self.agent(agent_obs).detach().cpu().numpy()\n",
    "                            a = np.argmax(net_out)\n",
    "\n",
    "                    agent_step, oponent_step = env.step(a)\n",
    "                    next_agent_obs, r, done, info = agent_step\n",
    "\n",
    "                    next_agent_obs = np.array(next_agent_obs).astype(np.uint8)\n",
    "                    \n",
    "                    if done:\n",
    "                        if r == 1:\n",
    "                            ws = ws + 1\n",
    "                        elif r == -1:\n",
    "                            ls = ls + 1\n",
    "                        else:\n",
    "                            ds = ds + 1\n",
    "\n",
    "                    r = r * 20  # winning or loosing is now +20 / -20\n",
    "\n",
    "                    if r == 0:  # to encourage agent to win fast\n",
    "                        r = r - 0.03\n",
    "\n",
    "                    #                     if a == 5:  # if action == bomb\n",
    "                    #                         bomb_timestep.append(timesteps)\n",
    "\n",
    "                    #                     for b_t in bomb_timestep:\n",
    "                    #                         if timesteps - b_t == 10 and not done:\n",
    "                    #                             bomb_timestep.remove(b_t)\n",
    "                    #                             r = r + 0.2\n",
    "                    ret = ret + r\n",
    "\n",
    "                    self.buffer.add(agent_obs, a, r, next_agent_obs, done, i)\n",
    "                    agent_obs = next_agent_obs\n",
    "\n",
    "                    timesteps = timesteps + 1\n",
    "\n",
    "                    if (\n",
    "                        self.buffer.length() > batch_size\n",
    "                        and self.buffer.length() > self.update_every\n",
    "                    ):\n",
    "                        self.agent.train()\n",
    "                        self.optimizer.zero_grad()\n",
    "\n",
    "                        (\n",
    "                            states,\n",
    "                            actions,\n",
    "                            rewards,\n",
    "                            next_states,\n",
    "                            dones,\n",
    "                        ) = self.buffer.sample_batch(self.batch_size)\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            next_state_preds = self.target_agent(next_states)\n",
    "\n",
    "                        q_values = rewards + self.gamma * (\n",
    "                            (1 - dones) * torch.max(next_state_preds, dim=1)[0]\n",
    "                        )\n",
    "\n",
    "                        state_preds = self.agent(states)\n",
    "\n",
    "                        mask = torch.nn.functional.one_hot(\n",
    "                            actions.to(torch.int64), num_classes=6\n",
    "                        ).bool()\n",
    "                        predictions = torch.masked_select(state_preds, mask)\n",
    "\n",
    "                        loss = self.loss(predictions, q_values)\n",
    "                        loss.backward()\n",
    "\n",
    "                        self.optimizer.step()\n",
    "                        losses.append(loss.item())\n",
    "\n",
    "                        if timesteps % update_every == 0:\n",
    "                            self.soft_update()\n",
    "\n",
    "                    if done:\n",
    "                        # if ret > 18.9:\n",
    "                        #     self.buffer.save_episode(i)\n",
    "\n",
    "                        if self.random_env:\n",
    "                            env = get_random_environment(self.oponent, self.random_env)\n",
    "                        agent_obs, opponent_obs = env.reset()\n",
    "\n",
    "                        pbar.set_description(\n",
    "                            f\"Episode: \\t{i}\\t{ret}\\tW/L/D:{ws}/{ls}/{ds}\\t{datetime.now().strftime('%Y_%m_%d-%H_%M_%S')}\"\n",
    "                        )\n",
    "                    returns.append(ret)\n",
    "\n",
    "                if i % 200 == 0:\n",
    "                    checkpoint_dict = {\n",
    "                        \"model_params\": self.agent.state_dict(),\n",
    "                        \"timesteps\": timesteps,\n",
    "                    }\n",
    "                    torch.save(\n",
    "                        checkpoint_dict,\n",
    "                        f\"./final_challenge/{self.name}-{datetime.now().strftime('%Y_%m_%d-%H_%M_%S')}.p\",\n",
    "                    )\n",
    "\n",
    "        return returns, losses, self.agent, timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2ff6de7-3397-4f9d-99b3-e0b6887aa768",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Curriculm:\n",
    "    name: str\n",
    "    oponent: Any\n",
    "    n_epochs: int\n",
    "    random_env: bool\n",
    "    run_till_above: bool\n",
    "    threshold: int = 105\n",
    "\n",
    "\n",
    "class CurriculumRunner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        curriculums,\n",
    "        agent,\n",
    "        learning_rate,\n",
    "        buffer_size,\n",
    "        eps_min,\n",
    "        eps_max,\n",
    "        eps_decay,\n",
    "        batch_size,\n",
    "        update_every,\n",
    "        gamma,\n",
    "        tau,\n",
    "    ):\n",
    "        self.curriculums = curriculums\n",
    "\n",
    "        self.agent = agent\n",
    "        self.target_agent = PommermanAgent(num_stack=5, num_actions=NUM_ACTIONS).to(device)\n",
    "        self.target_agent.load_state_dict(self.agent.state_dict())\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_max = eps_max\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.returns = []\n",
    "        self.losses = []\n",
    "\n",
    "        self.buffer = None\n",
    "\n",
    "    def run(self):\n",
    "        for curriculum in self.curriculums:\n",
    "            while True:\n",
    "                print(f\"Running: {curriculum.name}\")\n",
    "                self.buffer = ReplayBuffer(\n",
    "                    num_actions=NUM_ACTIONS, size=self.buffer_size\n",
    "                )\n",
    "                optimizer = optim.Adam(self.agent.parameters(), lr=self.learning_rate)\n",
    "\n",
    "                runner = Runner(\n",
    "                    name=curriculum.name,\n",
    "                    agent=self.agent,\n",
    "                    target_agent=self.target_agent,\n",
    "                    buffer=self.buffer,\n",
    "                    optimizer=optimizer,\n",
    "                    loss=torch.nn.MSELoss(),\n",
    "                    eps_min=self.eps_min,\n",
    "                    eps_max=self.eps_max,\n",
    "                    eps_decay=self.eps_decay,\n",
    "                    batch_size=self.batch_size,\n",
    "                    update_every=self.update_every,\n",
    "                    gamma=self.gamma,\n",
    "                    tau=self.tau,\n",
    "                    oponent=curriculum.oponent,\n",
    "                    random_env=curriculum.random_env,\n",
    "                )\n",
    "\n",
    "                rs, ls, agent, timesteps = runner.run(\n",
    "                    timesteps=0, n_epochs=curriculum.n_epochs\n",
    "                )\n",
    "\n",
    "                self.agent = agent\n",
    "                self.target_agent.load_state_dict(self.agent.state_dict())\n",
    "                \n",
    "\n",
    "                mean_returns = sum(rs) / len(rs)\n",
    "                mean_losses = sum(ls) / len(ls)\n",
    "\n",
    "                self.returns.append(mean_returns)\n",
    "                self.losses.append(mean_losses)\n",
    "\n",
    "                plt.plot(rs, label=\"returns\")\n",
    "                plt.plot(ls, label=\"mean_losses\", alpha=0.2)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                print(f\"action distribution -> {self.buffer.get_action_distribution()}\")\n",
    "\n",
    "                if curriculum.run_till_above == False:\n",
    "                    break\n",
    "                elif mean_returns >= curriculum.threshold:\n",
    "                    print(f\"Learned section with: {mean_returns} in {timesteps}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Retry with: {mean_returns} in {timesteps}\")\n",
    "\n",
    "        return self.returns, self.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34324ac8-a29f-4b9d-8821-42c23ea2d3ba",
   "metadata": {},
   "source": [
    "## Define Hyperparameters and initialize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc0ef1ac-bb97-4a30-a8dd-f69a43afbfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = [\n",
    "    # Curriculm(\n",
    "    #     name=\"new_static_oponent\",\n",
    "    #     oponent=static_agent,\n",
    "    #     n_epochs=2_000,\n",
    "    #     random_env=False,\n",
    "    #     run_till_above=False,\n",
    "    #     threshold=-5,\n",
    "    # ),\n",
    "    # Curriculm(\n",
    "    #     name=\"new_rand_no_bomb\",\n",
    "    #     oponent=rand_no_bomb_agent,\n",
    "    #     n_epochs=2_000,\n",
    "    #     random_env=False,\n",
    "    #     run_till_above=False,\n",
    "    #     threshold=-6,\n",
    "    # ),\n",
    "    # Curriculm(\n",
    "    #     name=\"new_f_rand_bomb\",\n",
    "    #     oponent=rand_agent,\n",
    "    #     n_epochs=1_000,\n",
    "    #     random_env=False,\n",
    "    #     run_till_above=True,\n",
    "    #     threshold=10,\n",
    "    # ),\n",
    "    Curriculm(\n",
    "        name=\"new_f_simple_bomb\",\n",
    "        oponent=None,\n",
    "        n_epochs=1_000,\n",
    "        random_env=False,\n",
    "        run_till_above=True,\n",
    "        threshold=10,\n",
    "    ),\n",
    "    # Curriculm(\n",
    "    #     name=\"static_oponent_random\",\n",
    "    #     oponent=static_agent,\n",
    "    #     n_epochs=10_000,\n",
    "    #     random_env=True\n",
    "    # ),\n",
    "    # Curriculm(\n",
    "    #     name=\"rand_no_bomb_random\",\n",
    "    #     oponent=rand_no_bomb_agent,\n",
    "    #     n_epochs=10_000,\n",
    "    #     random_env=True,\n",
    "    # ),\n",
    "    # Curriculm(\n",
    "    #     name=\"rand_bomb_random\",\n",
    "    #     oponent=rand_agent,\n",
    "    #     n_epochs=10_000,\n",
    "    #     random_env=True),\n",
    "]\n",
    "\n",
    "agent = PommermanAgent(num_stack=5, num_actions=NUM_ACTIONS).to(device)\n",
    "loaded = torch.load(\"final_challenge/new_f_simple_bomb-2022_06_20-23_20_45.p\", map_location=device)\n",
    "timesteps = loaded[\"timesteps\"]\n",
    "agent.load_state_dict(loaded[\"model_params\"])\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "buffer_size = 3_500\n",
    "\n",
    "eps_min = 0.0\n",
    "eps_max = 0.8\n",
    "eps_decay = 10_000\n",
    "batch_size = 256\n",
    "update_every = 4\n",
    "gamma = 0.99\n",
    "tau = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21c4ab86-cbbb-432b-8976-52cea53a0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculum_runner = CurriculumRunner(\n",
    "    curriculums=cs,\n",
    "    agent=agent,\n",
    "    learning_rate=learning_rate,\n",
    "    buffer_size=buffer_size,\n",
    "    eps_min=eps_min,\n",
    "    eps_max=eps_max,\n",
    "    eps_decay=eps_decay,\n",
    "    batch_size=batch_size,\n",
    "    update_every=update_every,\n",
    "    gamma=gamma,\n",
    "    tau=tau,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e02c4c43-87fb-4d09-8eab-07517f210471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: new_f_simple_bomb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6663b1981f449c790faa9a1488334c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10534/4140297551.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# (5, 56, 48) ERROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurriculum_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_10534/3305147393.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 rs, ls, agent, timesteps = runner.run(\n\u001b[0;32m---> 76\u001b[0;31m                     \u001b[0mtimesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurriculum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 )\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10534/1981045011.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, timesteps, n_epochs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mtimesteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mupdate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# (5, 56, 48) ERROR\n",
    "returns, losses = curriculum_runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0a61f47-f2d1-4357-b15a-82ba27edb0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphael/.local/share/virtualenvs/assignments-geAH-P9e/lib/python3.7/site-packages/ipykernel_launcher.py:29: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n"
     ]
    }
   ],
   "source": [
    "env = get_random_environment(None, False)\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# agent.eval()\n",
    "\n",
    "agent = PommermanAgent(num_stack=5, num_actions=NUM_ACTIONS).to(device)\n",
    "\n",
    "loaded = torch.load(\"final_challenge/new_f_simple_bomb-2022_06_20-19_53_22.p\", map_location=device)\n",
    "timesteps = loaded[\"timesteps\"]\n",
    "agent.load_state_dict(loaded[\"model_params\"])\n",
    "\n",
    "\n",
    "\n",
    "onnx_path = \"./submission_loaded_old.onnx\"\n",
    "state_for_onnx = np.array(obs, dtype=np.float32)\n",
    "torch.onnx.export(agent,\n",
    "                  torch.from_numpy(state_for_onnx).float(), # example model input\n",
    "                  onnx_path, # file path\n",
    "                  export_params=True, # save trained parameters\n",
    "                  opset_version=10,\n",
    "                  do_constant_folding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d703a-ae0d-40fb-8a40-3bcaf6a8e8e1",
   "metadata": {},
   "source": [
    "### plt.plot(curriculum_runner.losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
